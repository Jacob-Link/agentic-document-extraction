# ü§ñ Agentic Document Extraction API

## üìã Assignment Overview

This project was required to build a containerized FastAPI service using Browser Use + Gemini 2.5+ that accepts URLs and autonomously downloads solicitation PDFs to S3 storage. 

## üéØ Current Implementation Status

**Unable to get browser-use working for PDF downloads**, so I built the entire structure with a **"dummy" file extraction system** that demonstrates all components working together:

- ‚úÖ **Docker containerization**
- ‚úÖ **FastAPI endpoints**
- ‚úÖ **S3 integration**
- ‚úÖ **Browser-Use + Gemini integration**
- ‚úÖ **Complete technical stack skeleton**

## üîß Demo Functionality

Instead of PDF extraction, the current implementation:

1. **Takes the required API input format** (URL, S3 bucket, S3 prefix)
2. **Runs a demo browser task**: "Go to https://techcrunch.com, what is the main article presented - mainly about?"
3. **Prints the files which are in the specified S3 bucket**
4. **Showcases the complete technical stack structure working**

> üí° Once PDF extraction works, it's simply a **plug-in to the existing working structure**.

## üöÄ Quick Start

### Prerequisites
- Docker & Docker Compose
- Gemini API Key
- AWS S3 credentials (with correct access permissions)

### Setup
```bash
# Create .env file with required API keys
cat > .env << EOF
GEMINI_API_KEY=your_gemini_api_key_here
AWS_ACCESS_KEY_ID=your_aws_access_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_key_here
DEBUG=True
EOF

# Build and run
docker-compose up -d

# Test the API
curl -X POST http://localhost:8000/extract \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://caleprocure.ca.gov/event/0850/0000036230",
    "s3_bucket": "my-solicitations",
    "s3_prefix": "ecal/event-036230/"
  }'
```

## üõ†Ô∏è Technology Stack

| Component | Technology | Status |
|-----------|-----------|--------|
| üåê **Web Framework** | FastAPI 0.116.2 | ‚úÖ Working |
| ü§ñ **AI Agent** | Browser Use 0.7.8 + Gemini 2.5+ | ‚úÖ Working |
| ‚òÅÔ∏è **Storage** | AWS S3 (boto3 1.40.33) | ‚úÖ Working |
| üê≥ **Container** | Docker | ‚úÖ Working |
| üìä **Logging** | Structlog 25.4.0 | ‚úÖ Working |

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # üöÄ FastAPI application
‚îÇ   ‚îî‚îÄ‚îÄ models.py            # üìã Request/response models
‚îú‚îÄ‚îÄ agent/
‚îÇ   ‚îî‚îÄ‚îÄ document_extractor.py # ü§ñ Browser agent (with DEBUG mode)
‚îî‚îÄ‚îÄ s3/
    ‚îú‚îÄ‚îÄ s3_uploader.py       # ‚òÅÔ∏è S3 operations
    ‚îî‚îÄ‚îÄ s3_dummy.py          # üé≠ Dummy S3 for demo

tests/
‚îú‚îÄ‚îÄ test_gemini_key.py       # üîë Gemini API test
‚îî‚îÄ‚îÄ test_s3_access.py        # ü™£ S3 access test
```

## üéÆ API Demo

**Endpoint**: `POST /extract`

**Input**:
```json
{
  "url": "https://caleprocure.ca.gov/event/0850/0000036230",
  "s3_bucket": "my-solicitations",
  "s3_prefix": "ecal/event-036230/"
}
```

**Demo Output**:
```json
{
  "status": "success",
  "files": [
    "s3://my-solicitations/ecal/event-036230/demo-file-1.pdf",
    "s3://my-solicitations/ecal/event-036230/demo-file-2.pdf"
  ],
  "message": ">>> successfully extracted 2 documents"
}
```
## üîç What I Tried for PDF Extraction

1. **Naive Approach**: Explicitly instructed the agent to navigate to procurement pages and click download buttons directly
2. **Browser Configuration**: Modified Chrome args and browser preferences to disable PDF viewers and force automatic downloads
3. **Link Extraction Strategy**: Instead of manual clicking, extracted href links from download buttons and processed them programmatically
4. **Custom Tool Development**: Created and registered a custom download tool to the agent for triggering HTTP downloads from href links while maintaining browser session context

## üìö What I Have Learnt

* **Claude Code Excellence**: An incredible development tool - works best with focused, small task prompts. Let it run until achieving specific results. Running multiple terminals for non-overlapping tasks and tests maximizes efficiency.

* **Browser-Use Power & Limitations**: Powerful automation framework but lacks stability for certain complex tasks. The headless browser behavior can be unpredictable and requires deeper understanding.

* **Token Consumption Reality**: Over 6 million input tokens consumed rapidly when testing with Browser-Use + Gemini integration - cost considerations are crucial for production use.

## üöÄ What I Would Do Next (Given More Time & Resources)

### Technical Improvements
- **Upgrade to Claude Code Max** for enhanced capabilities and longer context windows
- **Investigate Browser-Use Pro**: Paid tier might offer more stable headless browser configurations and consistent behavior across devices
- **Deep Dive Learning**: Study Browser-Use architecture from fundamentals - appears learnable through systematic trial and error

---

*While I enjoyed this technical challenge, as of writing this im disappointed that the PDF extraction component could not be fully implemented within the given timeframe.*
